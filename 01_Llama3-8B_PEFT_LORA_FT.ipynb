{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189ae692-9f68-489e-bbb6-d0a4e9fbbce3",
   "metadata": {},
   "source": [
    "## Using Paramater Efficient Fine Tuning on Llama 3 with 8B Parameters on One Intel&reg; Gaudi&reg; 2 AI Accelerator\n",
    "This example will Fine Tune the Llama3 8B model using Parameter Efficient Fine Tuining (PEFT) and then run inference on a text prompt.  This will be using the Llama3-8B model with two task examples from the Optimum Habana library on the Hugging Face model repository.   The Optimum Habana library is optimized for Deep Learning training and inference on First-gen Gaudi and Gaudi2 and offers tasks such as text generation, language modeling, question answering and more. For all the examples and models, please refer to the [Optimum Habana GitHub](https://github.com/huggingface/optimum-habana#validated-models).\n",
    "\n",
    "This example will Fine Tune the Llama3-8B model using Parameter Efficient Fine Tuining (PEFT) on the timdettmers/openassistant-guanaco dataset using the Language-Modeling Task in Optimum Habana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac31ab-8a82-48c0-96e2-46a57c394b3f",
   "metadata": {},
   "source": [
    "### Parameter Efficient Fine Tuning with Low Rank Adaptation\n",
    "Parameter Efficient Fine Tuning is a strategy for adapting large pre-trained language models to specific tasks while minimizing computational and memory demands.   It aims to reduce the computational cost and memory requirements associated with fine-tuning large models while maintaining or even improving their performance.  It does so by adding a smaller task-specific layer, leveraging knowledge distillation, and often relying on few-shot learning, resulting in efficient yet effective models for various natural language understanding tasks.   PEFT starts with a pre-trained language model that has already learned a wide range of language understanding tasks from a large corpus of text data. These models are usually large and computationally expensive.   Instead of fine-tuning the entire pre-trained model, PEFT adds a task-specific layer or a few task-specific layers on top of the pre-trained model. These additional layers are relatively smaller and have fewer parameters compared to the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9921dd-897d-4754-95d5-d4eb913cd356",
   "metadata": {},
   "source": [
    "#### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5a36b-fd0c-4dec-b19e-178a5baf51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98207a-583e-48e7-b3cb-5c2418bfb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdefacb-bb33-4c23-b939-91c133dc2119",
   "metadata": {},
   "source": [
    "#### These versions are specific to 1.15 Synapse Gaudi SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cd31c-e0bd-4732-bfb6-eedd6ff57dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install peft==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2c2e4-f8a5-4beb-8402-a0b20068da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d73f7-8019-4657-81c4-12ac5de5aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repo_version(repo_path, expected_version):\n",
    "    \"\"\"\n",
    "    Checks the current version of the repository.\n",
    "\n",
    "    Parameters:\n",
    "    repo_path (str): The path to the repository.\n",
    "    expected_version (str): The expected version of the repository.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the repository is at the expected version, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Change to the repository directory\n",
    "        os.chdir(repo_path)\n",
    "        \n",
    "        # Get the current branch or tag\n",
    "        result = subprocess.run(['git', 'describe', '--tags'], capture_output=True, text=True)\n",
    "        current_version = result.stdout.strip()\n",
    "        \n",
    "        # Check if the current version matches the expected version\n",
    "        return current_version == expected_version\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking repository version: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        # Change back to the original directory\n",
    "        os.chdir('..')\n",
    "\n",
    "def clone_repo(repo_url, branch, repo_path):\n",
    "    \"\"\"\n",
    "    Clones the repository.\n",
    "\n",
    "    Parameters:\n",
    "    repo_url (str): The URL of the repository.\n",
    "    branch (str): The branch or tag to clone.\n",
    "    repo_path (str): The path to clone the repository into.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run(['git', 'clone', '-b', branch, repo_url, repo_path], check=True)\n",
    "        print(f\"Cloned repository from {repo_url} to {repo_path}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cloning repository: {e}\")\n",
    "\n",
    "def main():\n",
    "    repo_url = \"https://github.com/huggingface/optimum-habana.git\"\n",
    "    branch = \"v1.11.1\"\n",
    "    repo_path = \"optimum-habana\"\n",
    "\n",
    "    if os.path.exists(repo_path):\n",
    "        if check_repo_version(repo_path, branch):\n",
    "            print(f\"The repository at {repo_path} is already at version {branch}.\")\n",
    "        else:\n",
    "            print(f\"The repository at {repo_path} is not at version {branch}.\")\n",
    "            user_input = input(f\"Do you want to remove the directory {repo_path} and clone the correct version? (yes/no): \")\n",
    "            if user_input.lower() == 'yes':\n",
    "                shutil.rmtree(repo_path)\n",
    "                clone_repo(repo_url, branch, repo_path)\n",
    "            else:\n",
    "                print(\"The repository was not updated.\")\n",
    "    else:\n",
    "        clone_repo(repo_url, branch, repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836336ec-44ca-4441-80b3-1490cc03b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d446620-ddda-4ca2-8409-73fcc2fd1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ede335-a50d-4d80-976c-f00e445a8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please input your token--llama3 requires a license acceptance.\n",
    "#!huggingface-cli login --token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e37120-fde5-4f02-bbe8-de77ab56a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle Hugging Face Hub authentication\n",
    "def authenticate_huggingface():\n",
    "    try:\n",
    "        user_info = whoami()\n",
    "        print('Authorization token already provided')\n",
    "        print(f\"Logged in as: {user_info['name']}\")\n",
    "    except OSError:\n",
    "        notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bac610-8d2a-4e6e-b450-458c666fe2ac",
   "metadata": {},
   "source": [
    "## Fine Tuning the model with PEFT and LoRA\n",
    "\n",
    "We'll now run the fine tuning with the PEFT method. Remember that the PEFT methods only fine-tune a small number of extra model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.\n",
    "\n",
    "##### Here's a summary of the command required to run the Fine Tuning, you'll run this in the next cell below. \n",
    "Note in this case the following: \n",
    "1. Using the language modeling with LoRA; `run_lora_clm.py`\n",
    "2. It's very efficient: only 0.06% of the total paramters are being fine tuned of the total 8B parameters.\n",
    "4. Only 3 epochs are needed for fine tuning, it takes less than 20 minutes to run with the openassisant-guanaco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0a3b9-dea4-401e-baf7-1a642f799743",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 run_lora_clm.py \\\n",
    "    --model_name_or_path meta-llama/Meta-Llama-3-8B \\\n",
    "    --dataset_name timdettmers/openassistant-guanaco \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama3_8B_finetuned \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --warmup_ratio  0.03 \\\n",
    "    --lr_scheduler_type \"constant\" \\\n",
    "    --max_grad_norm  0.3 \\\n",
    "    --logging_steps 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --lora_rank=8 \\\n",
    "    --lora_alpha=16 \\\n",
    "    --lora_dropout=0.05 \\\n",
    "    --lora_target_modules \"q_proj\" \"v_proj\" \\\n",
    "    --dataset_concatenation \\\n",
    "    --max_seq_length 512 \\\n",
    "    --low_cpu_mem_usage True \\\n",
    "    --validation_split_percentage 4 \\\n",
    "    --adam_epsilon 1e-08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06f3b1-2a07-4910-86fe-0717313bcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25364cb-fa72-4550-bcec-a91a8764abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed07daf-efa2-4848-8258-7c306d15a2d8",
   "metadata": {},
   "source": [
    "# PEFT & LORA\n",
    "\n",
    "## Run the same prompt twice -- one that uses PEFT & LoRA and then the base model.  It takes about a minute to warm up and produce an answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb111a66-5dad-4333-89fa-50c31c2b5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the prompt above to compare between the PEFT and non-PEFT examples\n",
    "prompt = input(\"Enter a prompt for text generation: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f555971-6e31-4002-b3a9-2618e1e5b63f",
   "metadata": {},
   "source": [
    "## This is the bare bones run with no formatting uses only 100 tokens just to check functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb07ea-dc48-4476-abb5-1fae510f35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no formatting\n",
    "cmd = f'python3 run_generation.py --model_name_or_path meta-llama/Meta-Llama-3-8B --batch_size 1 --do_sample --max_new_tokens 100 --n_iterations 4 \\\n",
    "          --use_hpu_graphs --use_kv_cache --bf16 --prompt \"{prompt}\" \\\n",
    "          --peft_model ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama3_8B_finetuned'\n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99021093-70ff-4376-b07b-91eec69b7d90",
   "metadata": {},
   "source": [
    "### This is a formatted output using the PEFT and LoRA finetuned model on Gaudi 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aedcb-fdfd-4f16-a352-4543dc125c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to run the command and process the output\n",
    "def run_command(prompt, output_widget, status_widget):\n",
    "    cmd = f'python3 run_generation.py --model_name_or_path meta-llama/Meta-Llama-3-8B --batch_size 1 --do_sample --max_new_tokens 1000 --n_iterations 4 \\\n",
    "          --use_hpu_graphs --use_kv_cache --bf16 --prompt \"{prompt}\" \\\n",
    "          --peft_model ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/model_lora_llama3_8B_finetuned'\n",
    "    \n",
    "    # Update status to indicate processing\n",
    "    status_widget.value = \"Processing...\"\n",
    "    \n",
    "    # Run the command and capture the output\n",
    "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    # Stream the output\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        formatted_line = line.replace('\\\\n', '\\n').replace('### Human:', '\\n\\n### Human:').replace('### Assistant:', '\\n\\n### Assistant:')\n",
    "        output_widget.value += formatted_line\n",
    "    process.stdout.close()\n",
    "    process.wait()\n",
    "    \n",
    "    # Update status to indicate completion\n",
    "    status_widget.value = \"Completed\"\n",
    "\n",
    "# Create an input box for the prompt\n",
    "prompt_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a button to run the command\n",
    "run_button = widgets.Button(\n",
    "    description='Run Command',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to run the command',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create a text area to display the output\n",
    "output_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Output will be displayed here...',\n",
    "    description='Output:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='300px')\n",
    ")\n",
    "\n",
    "# Create a status widget to display the status\n",
    "status_label = widgets.Label(\n",
    "    value='',\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "# Define the function to handle button click\n",
    "def on_button_click(b):\n",
    "    prompt = prompt_input.value\n",
    "    output_area.value = ''  # Clear previous output\n",
    "    status_label.value = ''  # Clear previous status\n",
    "    thread = threading.Thread(target=run_command, args=(prompt, output_area, status_label))\n",
    "    thread.start()\n",
    "\n",
    "# Attach the button click event to the handler function\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display the input box, button, status label, and output area\n",
    "display(prompt_input, run_button, status_label, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf3577-90b5-496c-8194-b39d3dab80ab",
   "metadata": {},
   "source": [
    "# Non PEFT & LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693a5fb-a65d-416c-8f34-3b6ca4aae301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Llama-3-8B  catches the unformatted prompt from above\n",
    "cmd = f'python3 run_generation.py --model_name_or_path meta-llama/Meta-Llama-3-8B --batch_size 1 --do_sample --max_new_tokens 300 --n_iterations 4 \\\n",
    "          --use_hpu_graphs --use_kv_cache --bf16 --prompt \"{prompt}\"'\n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259a027-2b33-453c-bd12-78437e68c3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
